{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of a Signal that Produces Higgs Boson Particles and background signals\n",
    "# Convolutional Neural Network\n",
    "### Matthew Boyer and Jonah Goldfine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_curve, auc, confusion_matrix\n",
    "from sklearn.base import BaseEstimator,ClassifierMixin\n",
    "from sklearn.decomposition import PCA\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from tqdm import tqdm\n",
    "from skorch import NeuralNetBinaryClassifier\n",
    "from skorch import callbacks as cb\n",
    "import optuna\n",
    "name_dtype=np.array([['class_label', np.float32], ['jet_1_b-tag', np.float64],\n",
    "            ['jet_1_eta', np.float64], ['jet_1_phi', np.float64],\n",
    "            ['jet_1_pt', np.float64], ['jet_2_b-tag', np.float64],\n",
    "            ['jet_2_eta', np.float64], ['jet_2_phi', np.float64],\n",
    "            ['jet_2_pt', np.float64], ['jet_3_b-tag', np.float64],\n",
    "            ['jet_3_eta', np.float64], ['jet_3_phi', np.float64],\n",
    "            ['jet_3_pt', np.float64], ['jet_4_b-tag', np.float64],\n",
    "            ['jet_4_eta', np.float64], ['jet_4_phi', np.float64],\n",
    "            ['jet_4_pt', np.float64], ['lepton_eta', np.float64],\n",
    "            ['lepton_pT', np.float64], ['lepton_phi', np.float64],\n",
    "            ['m_bb', np.float64], ['m_jj', np.float64],\n",
    "            ['m_jjj', np.float64], ['m_jlv', np.float64],\n",
    "            ['m_lv', np.float64], ['m_wbb', np.float64],\n",
    "            ['m_wwbb', np.float64], ['missing_energy_magnitude', np.float64],\n",
    "            ['missing_energy_phi', np.float64]])\n",
    "fullData=pd.read_csv('HIGGS.csv',header=None,names=name_dtype[:,0])\n",
    "unscaled_X=fullData.drop(['class_label'],axis=1)\n",
    "scaler=StandardScaler()\n",
    "full_X=pd.DataFrame(scaler.fit_transform(unscaled_X.values),index=unscaled_X.index,columns=unscaled_X.columns)\n",
    "full_y=fullData['class_label']\n",
    "X_train_df,X_test_df,y_train_df,y_test_df=train_test_split(full_X,full_y,test_size=0.2,random_state=0)\n",
    "pca = PCA(n_components=0.95)  # retaining 95% of the variance\n",
    "X_train_pca = pca.fit_transform(X_train_df)\n",
    "X_test_pca = pca.transform(X_test_df)\n",
    "\n",
    "# Checking the number of components selected and the amount of variance explained\n",
    "n_components = pca.n_components_\n",
    "explained_variance = pca.explained_variance_ratio_.sum()\n",
    "\n",
    "X_train=torch.tensor(X_train_pca).float()\n",
    "X_test=torch.tensor(X_test_pca).float()\n",
    "y_train=torch.tensor(y_train_df.values).float()\n",
    "y_test=torch.tensor(y_test_df.values).float()\n",
    "class DNN_NoDrop(nn.Module):\n",
    "    def __init__(self, layer_sizes, activation):\n",
    "        super(DNN_NoDrop, self).__init__()\n",
    "        if layer_sizes == 'empty':\n",
    "            layer_sizes = []\n",
    "        else:\n",
    "            layer_sizes = [int(size) for size in layer_sizes.split('_')]\n",
    "\n",
    "        activation_functions = {'LeakyReLU': nn.LeakyReLU(), 'ReLU': nn.ReLU(), 'Tanh': nn.Tanh()}\n",
    "        activation = activation_functions[activation]\n",
    "        self.layers = nn.ModuleList()\n",
    "        input_size = 23\n",
    "        for hidden_size in layer_sizes:\n",
    "            self.layers.append(nn.Linear(input_size, hidden_size))\n",
    "            self.layers.append(activation)\n",
    "            input_size = hidden_size\n",
    "\n",
    "        self.layers.append(nn.Linear(input_size, 1))\n",
    "        self.layers.append(nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x.squeeze()\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "dataset = TensorDataset(X_train, y_train)\n",
    "\n",
    "# Split dataset into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "import time\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "def objective(trial):\n",
    "    start_time = time.time()\n",
    "    layer_sizes = trial.suggest_categorical('layer_sizes', ['empty', '16', '16_8', '64', '64_32', '64_32_16', '64_32_16_8', '128', '128_64', '128_64_32', '128_64_32_16', '128_64_32_16_8'])\n",
    "    activation = trial.suggest_categorical('activation', ['LeakyReLU', 'ReLU', 'Tanh'])\n",
    "    max_epochs = trial.suggest_categorical('max_epochs', [10, 25, 50, 100, 250])\n",
    "    batch_size = 704\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-1, log=True)\n",
    "    \n",
    "    model = DNN_NoDrop(layer_sizes=layer_sizes, activation=activation)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    model.to(device)\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=16)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=16)\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        for batch, (input, target) in enumerate(train_loader):\n",
    "            input, target = input.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with autocast():\n",
    "                output = model(input)\n",
    "                loss = criterion(output, target)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad(), autocast():\n",
    "        for input, target in val_loader:\n",
    "            input, target = input.to(device), target.to(device)\n",
    "            output = model(input)\n",
    "            val_loss += criterion(output, target).item()\n",
    "            pred = torch.sigmoid(output).ge(0.5).view(-1)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    accuracy = correct / len(val_loader.dataset)\n",
    "    trial.report(accuracy, epoch)\n",
    "\n",
    "    # Handle pruning based on the intermediate value\n",
    "    if trial.should_prune():\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    print(f\"Trial {trial.number} took {duration:.2f} seconds.\")\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial parameters: {'layer_sizes': '112_56_28_14_7', 'activation': 'LeakyReLU', 'max_epochs': 250, 'batch_size': 200, 'lr': 0.00010022659726287869}\n",
      "  epoch    accuracy    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ----------  ------------  -----------  ------------  -------\n",
      "      1      \u001b[36m0.6577\u001b[0m        \u001b[32m0.6135\u001b[0m       \u001b[35m0.6895\u001b[0m        \u001b[31m0.5854\u001b[0m  33.3308\n",
      "      2      \u001b[36m0.7018\u001b[0m        \u001b[32m0.5711\u001b[0m       \u001b[35m0.7075\u001b[0m        \u001b[31m0.5629\u001b[0m  33.2186\n",
      "      3      \u001b[36m0.7131\u001b[0m        \u001b[32m0.5548\u001b[0m       \u001b[35m0.7168\u001b[0m        \u001b[31m0.5498\u001b[0m  33.4217\n",
      "      4      \u001b[36m0.7202\u001b[0m        \u001b[32m0.5449\u001b[0m       \u001b[35m0.7216\u001b[0m        \u001b[31m0.5423\u001b[0m  33.1873\n",
      "      5      \u001b[36m0.7244\u001b[0m        \u001b[32m0.5386\u001b[0m       \u001b[35m0.7253\u001b[0m        \u001b[31m0.5371\u001b[0m  33.1697\n",
      "      6      \u001b[36m0.7279\u001b[0m        \u001b[32m0.5335\u001b[0m       \u001b[35m0.7281\u001b[0m        \u001b[31m0.5327\u001b[0m  33.2076\n",
      "      7      \u001b[36m0.7309\u001b[0m        \u001b[32m0.5290\u001b[0m       \u001b[35m0.7307\u001b[0m        \u001b[31m0.5290\u001b[0m  33.4589\n",
      "      8      \u001b[36m0.7335\u001b[0m        \u001b[32m0.5251\u001b[0m       \u001b[35m0.7329\u001b[0m        \u001b[31m0.5257\u001b[0m  33.0833\n",
      "      9      \u001b[36m0.7355\u001b[0m        \u001b[32m0.5218\u001b[0m       \u001b[35m0.7350\u001b[0m        \u001b[31m0.5229\u001b[0m  33.2814\n",
      "     10      \u001b[36m0.7372\u001b[0m        \u001b[32m0.5190\u001b[0m       \u001b[35m0.7361\u001b[0m        \u001b[31m0.5206\u001b[0m  33.3182\n",
      "     11      \u001b[36m0.7390\u001b[0m        \u001b[32m0.5166\u001b[0m       \u001b[35m0.7375\u001b[0m        \u001b[31m0.5185\u001b[0m  33.0533\n",
      "     12      \u001b[36m0.7404\u001b[0m        \u001b[32m0.5145\u001b[0m       \u001b[35m0.7388\u001b[0m        \u001b[31m0.5168\u001b[0m  33.0976\n",
      "     13      \u001b[36m0.7418\u001b[0m        \u001b[32m0.5126\u001b[0m       \u001b[35m0.7395\u001b[0m        \u001b[31m0.5153\u001b[0m  33.3232\n",
      "     14      \u001b[36m0.7429\u001b[0m        \u001b[32m0.5109\u001b[0m       \u001b[35m0.7405\u001b[0m        \u001b[31m0.5138\u001b[0m  33.0845\n",
      "     15      \u001b[36m0.7438\u001b[0m        \u001b[32m0.5094\u001b[0m       \u001b[35m0.7412\u001b[0m        \u001b[31m0.5125\u001b[0m  33.3679\n",
      "     16      \u001b[36m0.7446\u001b[0m        \u001b[32m0.5081\u001b[0m       \u001b[35m0.7422\u001b[0m        \u001b[31m0.5114\u001b[0m  33.2192\n",
      "     17      \u001b[36m0.7454\u001b[0m        \u001b[32m0.5069\u001b[0m       \u001b[35m0.7428\u001b[0m        \u001b[31m0.5105\u001b[0m  33.3223\n",
      "     18      \u001b[36m0.7460\u001b[0m        \u001b[32m0.5059\u001b[0m       \u001b[35m0.7436\u001b[0m        \u001b[31m0.5096\u001b[0m  33.0912\n",
      "     19      \u001b[36m0.7464\u001b[0m        \u001b[32m0.5049\u001b[0m       \u001b[35m0.7441\u001b[0m        \u001b[31m0.5088\u001b[0m  33.4119\n",
      "     20      \u001b[36m0.7471\u001b[0m        \u001b[32m0.5040\u001b[0m       \u001b[35m0.7446\u001b[0m        \u001b[31m0.5080\u001b[0m  33.1403\n",
      "     21      \u001b[36m0.7476\u001b[0m        \u001b[32m0.5032\u001b[0m       \u001b[35m0.7448\u001b[0m        \u001b[31m0.5073\u001b[0m  33.5221\n",
      "     22      \u001b[36m0.7480\u001b[0m        \u001b[32m0.5024\u001b[0m       \u001b[35m0.7453\u001b[0m        \u001b[31m0.5066\u001b[0m  32.9688\n",
      "     23      \u001b[36m0.7485\u001b[0m        \u001b[32m0.5017\u001b[0m       \u001b[35m0.7455\u001b[0m        \u001b[31m0.5062\u001b[0m  33.3719\n",
      "     24      \u001b[36m0.7490\u001b[0m        \u001b[32m0.5011\u001b[0m       \u001b[35m0.7458\u001b[0m        \u001b[31m0.5056\u001b[0m  33.4424\n",
      "     25      \u001b[36m0.7494\u001b[0m        \u001b[32m0.5005\u001b[0m       \u001b[35m0.7461\u001b[0m        \u001b[31m0.5052\u001b[0m  33.2093\n",
      "     26      \u001b[36m0.7497\u001b[0m        \u001b[32m0.4999\u001b[0m       \u001b[35m0.7465\u001b[0m        \u001b[31m0.5047\u001b[0m  33.3468\n",
      "     27      \u001b[36m0.7500\u001b[0m        \u001b[32m0.4993\u001b[0m       \u001b[35m0.7469\u001b[0m        \u001b[31m0.5041\u001b[0m  33.4303\n",
      "     28      \u001b[36m0.7504\u001b[0m        \u001b[32m0.4988\u001b[0m       \u001b[35m0.7473\u001b[0m        \u001b[31m0.5038\u001b[0m  33.2018\n",
      "     29      \u001b[36m0.7508\u001b[0m        \u001b[32m0.4983\u001b[0m       \u001b[35m0.7476\u001b[0m        \u001b[31m0.5033\u001b[0m  33.5672\n",
      "     30      \u001b[36m0.7509\u001b[0m        \u001b[32m0.4979\u001b[0m       \u001b[35m0.7478\u001b[0m        \u001b[31m0.5030\u001b[0m  33.3906\n",
      "     31      \u001b[36m0.7514\u001b[0m        \u001b[32m0.4974\u001b[0m       \u001b[35m0.7479\u001b[0m        \u001b[31m0.5028\u001b[0m  33.2646\n",
      "     32      \u001b[36m0.7516\u001b[0m        \u001b[32m0.4970\u001b[0m       \u001b[35m0.7480\u001b[0m        \u001b[31m0.5025\u001b[0m  33.2547\n",
      "     33      \u001b[36m0.7519\u001b[0m        \u001b[32m0.4967\u001b[0m       \u001b[35m0.7481\u001b[0m        \u001b[31m0.5024\u001b[0m  33.6310\n",
      "     34      \u001b[36m0.7522\u001b[0m        \u001b[32m0.4963\u001b[0m       \u001b[35m0.7483\u001b[0m        \u001b[31m0.5023\u001b[0m  34.8152\n",
      "     35      \u001b[36m0.7524\u001b[0m        \u001b[32m0.4959\u001b[0m       \u001b[35m0.7487\u001b[0m        \u001b[31m0.5019\u001b[0m  33.9008\n",
      "     36      \u001b[36m0.7526\u001b[0m        \u001b[32m0.4956\u001b[0m       \u001b[35m0.7488\u001b[0m        \u001b[31m0.5017\u001b[0m  34.2353\n",
      "     37      \u001b[36m0.7528\u001b[0m        \u001b[32m0.4952\u001b[0m       0.7488        \u001b[31m0.5015\u001b[0m  33.4609\n",
      "     38      \u001b[36m0.7531\u001b[0m        \u001b[32m0.4949\u001b[0m       \u001b[35m0.7491\u001b[0m        \u001b[31m0.5013\u001b[0m  33.3208\n",
      "     39      \u001b[36m0.7533\u001b[0m        \u001b[32m0.4946\u001b[0m       \u001b[35m0.7492\u001b[0m        \u001b[31m0.5011\u001b[0m  33.4447\n",
      "     40      \u001b[36m0.7535\u001b[0m        \u001b[32m0.4943\u001b[0m       \u001b[35m0.7494\u001b[0m        \u001b[31m0.5009\u001b[0m  33.3671\n",
      "     41      \u001b[36m0.7538\u001b[0m        \u001b[32m0.4940\u001b[0m       \u001b[35m0.7497\u001b[0m        \u001b[31m0.5008\u001b[0m  33.7089\n",
      "     42      \u001b[36m0.7539\u001b[0m        \u001b[32m0.4937\u001b[0m       \u001b[35m0.7498\u001b[0m        \u001b[31m0.5004\u001b[0m  33.5423\n",
      "     43      \u001b[36m0.7541\u001b[0m        \u001b[32m0.4934\u001b[0m       \u001b[35m0.7499\u001b[0m        \u001b[31m0.5003\u001b[0m  33.4545\n",
      "     44      \u001b[36m0.7543\u001b[0m        \u001b[32m0.4932\u001b[0m       \u001b[35m0.7501\u001b[0m        \u001b[31m0.5001\u001b[0m  33.3203\n",
      "     45      \u001b[36m0.7545\u001b[0m        \u001b[32m0.4929\u001b[0m       \u001b[35m0.7502\u001b[0m        \u001b[31m0.4999\u001b[0m  33.5867\n",
      "     46      \u001b[36m0.7546\u001b[0m        \u001b[32m0.4927\u001b[0m       \u001b[35m0.7502\u001b[0m        \u001b[31m0.4997\u001b[0m  33.8412\n",
      "     47      \u001b[36m0.7548\u001b[0m        \u001b[32m0.4924\u001b[0m       \u001b[35m0.7504\u001b[0m        \u001b[31m0.4995\u001b[0m  33.6954\n",
      "     48      \u001b[36m0.7550\u001b[0m        \u001b[32m0.4922\u001b[0m       \u001b[35m0.7506\u001b[0m        \u001b[31m0.4992\u001b[0m  33.7670\n",
      "     49      \u001b[36m0.7553\u001b[0m        \u001b[32m0.4919\u001b[0m       \u001b[35m0.7506\u001b[0m        \u001b[31m0.4991\u001b[0m  33.6493\n",
      "     50      \u001b[36m0.7554\u001b[0m        \u001b[32m0.4917\u001b[0m       \u001b[35m0.7510\u001b[0m        \u001b[31m0.4988\u001b[0m  33.8705\n",
      "     51      \u001b[36m0.7555\u001b[0m        \u001b[32m0.4915\u001b[0m       \u001b[35m0.7512\u001b[0m        \u001b[31m0.4986\u001b[0m  33.7807\n",
      "     52      \u001b[36m0.7556\u001b[0m        \u001b[32m0.4913\u001b[0m       0.7510        \u001b[31m0.4984\u001b[0m  33.8562\n",
      "     53      \u001b[36m0.7558\u001b[0m        \u001b[32m0.4910\u001b[0m       \u001b[35m0.7512\u001b[0m        \u001b[31m0.4983\u001b[0m  33.8179\n",
      "     54      \u001b[36m0.7559\u001b[0m        \u001b[32m0.4908\u001b[0m       \u001b[35m0.7515\u001b[0m        \u001b[31m0.4982\u001b[0m  33.9671\n",
      "     55      \u001b[36m0.7560\u001b[0m        \u001b[32m0.4906\u001b[0m       \u001b[35m0.7517\u001b[0m        \u001b[31m0.4978\u001b[0m  34.7171\n",
      "     56      \u001b[36m0.7562\u001b[0m        \u001b[32m0.4904\u001b[0m       \u001b[35m0.7518\u001b[0m        \u001b[31m0.4977\u001b[0m  34.0144\n",
      "     57      \u001b[36m0.7563\u001b[0m        \u001b[32m0.4902\u001b[0m       \u001b[35m0.7520\u001b[0m        \u001b[31m0.4976\u001b[0m  34.0551\n",
      "     58      \u001b[36m0.7565\u001b[0m        \u001b[32m0.4900\u001b[0m       \u001b[35m0.7520\u001b[0m        \u001b[31m0.4974\u001b[0m  33.2860\n",
      "     59      \u001b[36m0.7566\u001b[0m        \u001b[32m0.4898\u001b[0m       0.7520        \u001b[31m0.4974\u001b[0m  33.2596\n",
      "     60      \u001b[36m0.7568\u001b[0m        \u001b[32m0.4896\u001b[0m       0.7518        \u001b[31m0.4973\u001b[0m  33.3637\n",
      "     61      \u001b[36m0.7568\u001b[0m        \u001b[32m0.4895\u001b[0m       0.7519        \u001b[31m0.4971\u001b[0m  33.9187\n",
      "     62      \u001b[36m0.7570\u001b[0m        \u001b[32m0.4893\u001b[0m       \u001b[35m0.7521\u001b[0m        \u001b[31m0.4971\u001b[0m  33.5873\n",
      "     63      \u001b[36m0.7572\u001b[0m        \u001b[32m0.4891\u001b[0m       \u001b[35m0.7521\u001b[0m        \u001b[31m0.4970\u001b[0m  34.0771\n",
      "     64      \u001b[36m0.7573\u001b[0m        \u001b[32m0.4889\u001b[0m       \u001b[35m0.7521\u001b[0m        \u001b[31m0.4969\u001b[0m  33.6861\n",
      "     65      \u001b[36m0.7573\u001b[0m        \u001b[32m0.4887\u001b[0m       \u001b[35m0.7522\u001b[0m        \u001b[31m0.4967\u001b[0m  33.5544\n",
      "     66      \u001b[36m0.7574\u001b[0m        \u001b[32m0.4886\u001b[0m       \u001b[35m0.7524\u001b[0m        \u001b[31m0.4966\u001b[0m  33.4595\n",
      "     67      \u001b[36m0.7575\u001b[0m        \u001b[32m0.4884\u001b[0m       0.7523        \u001b[31m0.4964\u001b[0m  33.7067\n",
      "     68      \u001b[36m0.7576\u001b[0m        \u001b[32m0.4882\u001b[0m       0.7523        \u001b[31m0.4964\u001b[0m  33.6068\n",
      "     69      \u001b[36m0.7577\u001b[0m        \u001b[32m0.4881\u001b[0m       0.7523        \u001b[31m0.4963\u001b[0m  33.7507\n",
      "     70      \u001b[36m0.7578\u001b[0m        \u001b[32m0.4879\u001b[0m       0.7523        \u001b[31m0.4961\u001b[0m  33.4291\n",
      "     71      \u001b[36m0.7579\u001b[0m        \u001b[32m0.4877\u001b[0m       0.7524        0.4961  33.5620\n",
      "     72      \u001b[36m0.7581\u001b[0m        \u001b[32m0.4876\u001b[0m       \u001b[35m0.7527\u001b[0m        \u001b[31m0.4960\u001b[0m  32.8986\n",
      "     73      \u001b[36m0.7582\u001b[0m        \u001b[32m0.4874\u001b[0m       0.7525        \u001b[31m0.4959\u001b[0m  32.9185\n",
      "     74      \u001b[36m0.7583\u001b[0m        \u001b[32m0.4873\u001b[0m       0.7526        \u001b[31m0.4959\u001b[0m  33.2974\n",
      "     75      \u001b[36m0.7584\u001b[0m        \u001b[32m0.4872\u001b[0m       \u001b[35m0.7528\u001b[0m        \u001b[31m0.4958\u001b[0m  33.1602\n",
      "     76      \u001b[36m0.7585\u001b[0m        \u001b[32m0.4870\u001b[0m       \u001b[35m0.7528\u001b[0m        \u001b[31m0.4957\u001b[0m  32.7573\n",
      "     77      \u001b[36m0.7587\u001b[0m        \u001b[32m0.4869\u001b[0m       \u001b[35m0.7529\u001b[0m        \u001b[31m0.4957\u001b[0m  33.2983\n",
      "     78      \u001b[36m0.7587\u001b[0m        \u001b[32m0.4868\u001b[0m       0.7527        \u001b[31m0.4957\u001b[0m  33.6762\n",
      "     79      \u001b[36m0.7589\u001b[0m        \u001b[32m0.4866\u001b[0m       \u001b[35m0.7530\u001b[0m        \u001b[31m0.4955\u001b[0m  33.0554\n",
      "     80      \u001b[36m0.7590\u001b[0m        \u001b[32m0.4865\u001b[0m       0.7529        \u001b[31m0.4954\u001b[0m  33.4038\n",
      "     81      \u001b[36m0.7590\u001b[0m        \u001b[32m0.4864\u001b[0m       0.7528        \u001b[31m0.4953\u001b[0m  33.0788\n",
      "     82      \u001b[36m0.7591\u001b[0m        \u001b[32m0.4862\u001b[0m       0.7529        \u001b[31m0.4952\u001b[0m  33.1924\n",
      "     83      \u001b[36m0.7592\u001b[0m        \u001b[32m0.4861\u001b[0m       \u001b[35m0.7530\u001b[0m        \u001b[31m0.4951\u001b[0m  33.2223\n",
      "     84      \u001b[36m0.7592\u001b[0m        \u001b[32m0.4860\u001b[0m       0.7530        0.4952  33.1058\n",
      "     85      \u001b[36m0.7594\u001b[0m        \u001b[32m0.4858\u001b[0m       \u001b[35m0.7531\u001b[0m        \u001b[31m0.4949\u001b[0m  33.0031\n",
      "     86      0.7594        \u001b[32m0.4857\u001b[0m       0.7530        0.4950  33.1707\n",
      "     87      \u001b[36m0.7595\u001b[0m        \u001b[32m0.4856\u001b[0m       \u001b[35m0.7534\u001b[0m        \u001b[31m0.4948\u001b[0m  33.2052\n",
      "     88      \u001b[36m0.7596\u001b[0m        \u001b[32m0.4854\u001b[0m       \u001b[35m0.7534\u001b[0m        \u001b[31m0.4947\u001b[0m  33.2935\n",
      "     89      \u001b[36m0.7597\u001b[0m        \u001b[32m0.4853\u001b[0m       \u001b[35m0.7534\u001b[0m        \u001b[31m0.4946\u001b[0m  33.1718\n",
      "     90      \u001b[36m0.7598\u001b[0m        \u001b[32m0.4852\u001b[0m       \u001b[35m0.7535\u001b[0m        \u001b[31m0.4944\u001b[0m  32.9304\n",
      "     91      \u001b[36m0.7598\u001b[0m        \u001b[32m0.4851\u001b[0m       0.7533        \u001b[31m0.4944\u001b[0m  32.9716\n",
      "     92      \u001b[36m0.7600\u001b[0m        \u001b[32m0.4849\u001b[0m       0.7535        \u001b[31m0.4943\u001b[0m  33.4579\n",
      "     93      \u001b[36m0.7601\u001b[0m        \u001b[32m0.4848\u001b[0m       \u001b[35m0.7535\u001b[0m        0.4943  32.8633\n",
      "     94      \u001b[36m0.7601\u001b[0m        \u001b[32m0.4847\u001b[0m       0.7534        \u001b[31m0.4943\u001b[0m  33.0351\n",
      "     95      \u001b[36m0.7603\u001b[0m        \u001b[32m0.4846\u001b[0m       \u001b[35m0.7535\u001b[0m        \u001b[31m0.4942\u001b[0m  33.0710\n",
      "     96      \u001b[36m0.7603\u001b[0m        \u001b[32m0.4845\u001b[0m       \u001b[35m0.7537\u001b[0m        \u001b[31m0.4941\u001b[0m  33.2753\n",
      "     97      \u001b[36m0.7604\u001b[0m        \u001b[32m0.4843\u001b[0m       \u001b[35m0.7537\u001b[0m        \u001b[31m0.4940\u001b[0m  32.7136\n",
      "     98      \u001b[36m0.7604\u001b[0m        \u001b[32m0.4842\u001b[0m       \u001b[35m0.7538\u001b[0m        \u001b[31m0.4939\u001b[0m  33.3731\n",
      "     99      \u001b[36m0.7606\u001b[0m        \u001b[32m0.4841\u001b[0m       0.7538        \u001b[31m0.4938\u001b[0m  32.7607\n",
      "    100      \u001b[36m0.7606\u001b[0m        \u001b[32m0.4840\u001b[0m       \u001b[35m0.7539\u001b[0m        0.4939  33.3309\n",
      "    101      \u001b[36m0.7607\u001b[0m        \u001b[32m0.4839\u001b[0m       \u001b[35m0.7539\u001b[0m        \u001b[31m0.4937\u001b[0m  33.0932\n",
      "    102      \u001b[36m0.7607\u001b[0m        \u001b[32m0.4838\u001b[0m       \u001b[35m0.7541\u001b[0m        0.4937  33.2370\n",
      "    103      \u001b[36m0.7609\u001b[0m        \u001b[32m0.4837\u001b[0m       \u001b[35m0.7542\u001b[0m        \u001b[31m0.4936\u001b[0m  33.5429\n",
      "    104      0.7609        \u001b[32m0.4836\u001b[0m       0.7542        \u001b[31m0.4935\u001b[0m  33.3273\n",
      "    105      \u001b[36m0.7609\u001b[0m        \u001b[32m0.4835\u001b[0m       0.7542        \u001b[31m0.4934\u001b[0m  33.0516\n",
      "    106      \u001b[36m0.7611\u001b[0m        \u001b[32m0.4834\u001b[0m       0.7541        0.4934  33.6356\n",
      "    107      0.7610        \u001b[32m0.4833\u001b[0m       0.7541        0.4934  32.9384\n",
      "    108      \u001b[36m0.7611\u001b[0m        \u001b[32m0.4832\u001b[0m       0.7540        \u001b[31m0.4932\u001b[0m  33.1146\n",
      "    109      \u001b[36m0.7612\u001b[0m        \u001b[32m0.4831\u001b[0m       \u001b[35m0.7543\u001b[0m        \u001b[31m0.4930\u001b[0m  33.2682\n",
      "    110      \u001b[36m0.7612\u001b[0m        \u001b[32m0.4830\u001b[0m       0.7543        \u001b[31m0.4930\u001b[0m  33.1630\n",
      "    111      \u001b[36m0.7613\u001b[0m        \u001b[32m0.4829\u001b[0m       \u001b[35m0.7543\u001b[0m        \u001b[31m0.4929\u001b[0m  33.2248\n",
      "    112      \u001b[36m0.7613\u001b[0m        \u001b[32m0.4828\u001b[0m       0.7542        \u001b[31m0.4928\u001b[0m  33.3828\n",
      "    113      \u001b[36m0.7614\u001b[0m        \u001b[32m0.4827\u001b[0m       0.7543        0.4928  33.0257\n",
      "    114      \u001b[36m0.7615\u001b[0m        \u001b[32m0.4826\u001b[0m       \u001b[35m0.7544\u001b[0m        \u001b[31m0.4927\u001b[0m  33.2054\n",
      "    115      \u001b[36m0.7616\u001b[0m        \u001b[32m0.4825\u001b[0m       \u001b[35m0.7545\u001b[0m        \u001b[31m0.4927\u001b[0m  33.1183\n",
      "    116      \u001b[36m0.7617\u001b[0m        \u001b[32m0.4824\u001b[0m       \u001b[35m0.7545\u001b[0m        \u001b[31m0.4926\u001b[0m  32.9711\n",
      "    117      \u001b[36m0.7617\u001b[0m        \u001b[32m0.4823\u001b[0m       \u001b[35m0.7547\u001b[0m        \u001b[31m0.4926\u001b[0m  33.3141\n",
      "    118      \u001b[36m0.7617\u001b[0m        \u001b[32m0.4822\u001b[0m       \u001b[35m0.7548\u001b[0m        \u001b[31m0.4924\u001b[0m  33.2509\n",
      "    119      \u001b[36m0.7618\u001b[0m        \u001b[32m0.4821\u001b[0m       0.7548        0.4926  33.1198\n",
      "    120      \u001b[36m0.7619\u001b[0m        \u001b[32m0.4820\u001b[0m       0.7546        0.4925  33.2495\n",
      "    121      \u001b[36m0.7619\u001b[0m        \u001b[32m0.4820\u001b[0m       0.7547        0.4924  33.1087\n",
      "    122      \u001b[36m0.7620\u001b[0m        \u001b[32m0.4819\u001b[0m       0.7548        \u001b[31m0.4924\u001b[0m  32.8823\n",
      "    123      \u001b[36m0.7620\u001b[0m        \u001b[32m0.4818\u001b[0m       0.7547        0.4925  33.2705\n",
      "    124      \u001b[36m0.7621\u001b[0m        \u001b[32m0.4817\u001b[0m       \u001b[35m0.7549\u001b[0m        \u001b[31m0.4923\u001b[0m  33.1206\n",
      "    125      0.7621        \u001b[32m0.4816\u001b[0m       0.7548        \u001b[31m0.4923\u001b[0m  33.3332\n",
      "    126      0.7621        \u001b[32m0.4815\u001b[0m       \u001b[35m0.7549\u001b[0m        \u001b[31m0.4922\u001b[0m  33.1510\n",
      "    127      \u001b[36m0.7622\u001b[0m        \u001b[32m0.4814\u001b[0m       \u001b[35m0.7551\u001b[0m        \u001b[31m0.4921\u001b[0m  33.5232\n",
      "    128      \u001b[36m0.7623\u001b[0m        \u001b[32m0.4814\u001b[0m       \u001b[35m0.7552\u001b[0m        \u001b[31m0.4921\u001b[0m  32.9239\n",
      "    129      \u001b[36m0.7623\u001b[0m        \u001b[32m0.4813\u001b[0m       0.7552        0.4921  33.5846\n",
      "    130      \u001b[36m0.7625\u001b[0m        \u001b[32m0.4812\u001b[0m       \u001b[35m0.7554\u001b[0m        \u001b[31m0.4919\u001b[0m  33.0912\n",
      "    131      \u001b[36m0.7625\u001b[0m        \u001b[32m0.4811\u001b[0m       \u001b[35m0.7554\u001b[0m        \u001b[31m0.4919\u001b[0m  33.1003\n",
      "    132      \u001b[36m0.7626\u001b[0m        \u001b[32m0.4811\u001b[0m       \u001b[35m0.7556\u001b[0m        \u001b[31m0.4918\u001b[0m  33.0466\n",
      "    133      \u001b[36m0.7626\u001b[0m        \u001b[32m0.4810\u001b[0m       0.7554        0.4918  33.2930\n",
      "    134      \u001b[36m0.7627\u001b[0m        \u001b[32m0.4809\u001b[0m       \u001b[35m0.7558\u001b[0m        \u001b[31m0.4916\u001b[0m  32.9677\n",
      "    135      \u001b[36m0.7627\u001b[0m        \u001b[32m0.4808\u001b[0m       0.7557        \u001b[31m0.4916\u001b[0m  33.5999\n",
      "    136      0.7626        \u001b[32m0.4808\u001b[0m       0.7556        0.4917  33.0545\n",
      "    137      \u001b[36m0.7627\u001b[0m        \u001b[32m0.4807\u001b[0m       0.7557        \u001b[31m0.4915\u001b[0m  33.3725\n",
      "    138      \u001b[36m0.7628\u001b[0m        \u001b[32m0.4806\u001b[0m       \u001b[35m0.7558\u001b[0m        \u001b[31m0.4914\u001b[0m  33.4217\n",
      "    139      \u001b[36m0.7628\u001b[0m        \u001b[32m0.4805\u001b[0m       0.7558        0.4915  33.1685\n",
      "    140      \u001b[36m0.7629\u001b[0m        \u001b[32m0.4805\u001b[0m       \u001b[35m0.7559\u001b[0m        0.4915  33.1362\n",
      "    141      \u001b[36m0.7629\u001b[0m        \u001b[32m0.4804\u001b[0m       \u001b[35m0.7560\u001b[0m        \u001b[31m0.4914\u001b[0m  33.5220\n",
      "    142      \u001b[36m0.7630\u001b[0m        \u001b[32m0.4803\u001b[0m       0.7558        \u001b[31m0.4914\u001b[0m  33.0402\n",
      "    143      \u001b[36m0.7630\u001b[0m        \u001b[32m0.4803\u001b[0m       0.7559        \u001b[31m0.4913\u001b[0m  33.6703\n",
      "    144      0.7630        \u001b[32m0.4802\u001b[0m       0.7559        \u001b[31m0.4913\u001b[0m  33.4445\n",
      "    145      \u001b[36m0.7631\u001b[0m        \u001b[32m0.4801\u001b[0m       \u001b[35m0.7560\u001b[0m        \u001b[31m0.4913\u001b[0m  33.4219\n",
      "    146      \u001b[36m0.7631\u001b[0m        \u001b[32m0.4801\u001b[0m       0.7560        \u001b[31m0.4912\u001b[0m  33.3156\n",
      "    147      \u001b[36m0.7632\u001b[0m        \u001b[32m0.4800\u001b[0m       0.7559        \u001b[31m0.4911\u001b[0m  33.1754\n",
      "    148      0.7632        \u001b[32m0.4799\u001b[0m       \u001b[35m0.7561\u001b[0m        \u001b[31m0.4911\u001b[0m  33.4129\n",
      "    149      \u001b[36m0.7633\u001b[0m        \u001b[32m0.4799\u001b[0m       0.7560        0.4912  33.5381\n",
      "    150      \u001b[36m0.7633\u001b[0m        \u001b[32m0.4798\u001b[0m       \u001b[35m0.7562\u001b[0m        \u001b[31m0.4911\u001b[0m  33.1344\n",
      "    151      0.7633        \u001b[32m0.4797\u001b[0m       \u001b[35m0.7562\u001b[0m        \u001b[31m0.4910\u001b[0m  33.2029\n",
      "    152      \u001b[36m0.7633\u001b[0m        \u001b[32m0.4797\u001b[0m       0.7561        0.4910  33.0906\n",
      "    153      \u001b[36m0.7634\u001b[0m        \u001b[32m0.4796\u001b[0m       0.7561        \u001b[31m0.4910\u001b[0m  33.1365\n",
      "    154      \u001b[36m0.7635\u001b[0m        \u001b[32m0.4796\u001b[0m       0.7560        \u001b[31m0.4910\u001b[0m  33.5121\n",
      "    155      0.7635        \u001b[32m0.4795\u001b[0m       0.7561        0.4910  33.3896\n",
      "    156      \u001b[36m0.7635\u001b[0m        \u001b[32m0.4795\u001b[0m       0.7562        0.4910  33.0809\n",
      "    157      \u001b[36m0.7636\u001b[0m        \u001b[32m0.4794\u001b[0m       0.7561        0.4910  33.2163\n",
      "    158      \u001b[36m0.7636\u001b[0m        \u001b[32m0.4794\u001b[0m       0.7561        0.4910  33.3013\n",
      "    159      0.7636        \u001b[32m0.4793\u001b[0m       0.7561        \u001b[31m0.4909\u001b[0m  33.0286\n",
      "    160      \u001b[36m0.7637\u001b[0m        \u001b[32m0.4792\u001b[0m       0.7561        \u001b[31m0.4908\u001b[0m  33.5756\n",
      "    161      \u001b[36m0.7637\u001b[0m        \u001b[32m0.4792\u001b[0m       \u001b[35m0.7562\u001b[0m        0.4909  33.0609\n",
      "    162      \u001b[36m0.7638\u001b[0m        \u001b[32m0.4791\u001b[0m       0.7561        \u001b[31m0.4908\u001b[0m  34.2981\n",
      "    163      \u001b[36m0.7639\u001b[0m        \u001b[32m0.4791\u001b[0m       0.7562        \u001b[31m0.4907\u001b[0m  34.2185\n",
      "    164      0.7638        \u001b[32m0.4790\u001b[0m       0.7562        0.4908  34.2539\n",
      "    165      \u001b[36m0.7639\u001b[0m        \u001b[32m0.4790\u001b[0m       0.7562        0.4908  34.7693\n",
      "    166      \u001b[36m0.7639\u001b[0m        \u001b[32m0.4789\u001b[0m       0.7561        0.4908  36.0364\n",
      "    167      \u001b[36m0.7640\u001b[0m        \u001b[32m0.4789\u001b[0m       0.7562        0.4908  36.1572\n",
      "    168      \u001b[36m0.7641\u001b[0m        \u001b[32m0.4788\u001b[0m       0.7560        0.4908  36.1592\n",
      "    169      \u001b[36m0.7641\u001b[0m        \u001b[32m0.4788\u001b[0m       0.7561        \u001b[31m0.4907\u001b[0m  35.9536\n",
      "    170      0.7640        \u001b[32m0.4787\u001b[0m       0.7559        0.4908  35.9349\n",
      "    171      0.7641        \u001b[32m0.4786\u001b[0m       0.7561        \u001b[31m0.4907\u001b[0m  36.0666\n",
      "    172      \u001b[36m0.7642\u001b[0m        \u001b[32m0.4786\u001b[0m       0.7560        0.4907  35.9308\n",
      "    173      \u001b[36m0.7642\u001b[0m        \u001b[32m0.4785\u001b[0m       0.7561        \u001b[31m0.4907\u001b[0m  36.2332\n",
      "    174      \u001b[36m0.7643\u001b[0m        \u001b[32m0.4785\u001b[0m       0.7560        \u001b[31m0.4906\u001b[0m  36.2272\n",
      "    175      0.7643        \u001b[32m0.4784\u001b[0m       0.7561        0.4906  35.8607\n",
      "    176      \u001b[36m0.7643\u001b[0m        \u001b[32m0.4784\u001b[0m       0.7562        0.4906  36.3572\n",
      "    177      \u001b[36m0.7644\u001b[0m        \u001b[32m0.4783\u001b[0m       0.7561        \u001b[31m0.4906\u001b[0m  36.0230\n",
      "    178      0.7644        \u001b[32m0.4783\u001b[0m       0.7561        \u001b[31m0.4905\u001b[0m  36.1796\n",
      "    179      \u001b[36m0.7644\u001b[0m        \u001b[32m0.4782\u001b[0m       0.7562        0.4906  36.1551\n",
      "    180      0.7644        \u001b[32m0.4782\u001b[0m       0.7562        0.4905  35.8282\n",
      "    181      \u001b[36m0.7644\u001b[0m        \u001b[32m0.4781\u001b[0m       0.7562        0.4906  36.2939\n",
      "    182      \u001b[36m0.7645\u001b[0m        \u001b[32m0.4781\u001b[0m       0.7561        \u001b[31m0.4905\u001b[0m  36.0766\n",
      "    183      0.7645        \u001b[32m0.4781\u001b[0m       0.7562        \u001b[31m0.4905\u001b[0m  36.0645\n",
      "    184      \u001b[36m0.7645\u001b[0m        \u001b[32m0.4780\u001b[0m       0.7561        \u001b[31m0.4903\u001b[0m  36.1515\n",
      "    185      \u001b[36m0.7645\u001b[0m        \u001b[32m0.4780\u001b[0m       0.7561        0.4904  35.7540\n",
      "    186      \u001b[36m0.7646\u001b[0m        \u001b[32m0.4779\u001b[0m       0.7562        \u001b[31m0.4903\u001b[0m  36.0516\n",
      "    187      0.7646        \u001b[32m0.4779\u001b[0m       \u001b[35m0.7563\u001b[0m        0.4903  36.1428\n",
      "    188      \u001b[36m0.7646\u001b[0m        \u001b[32m0.4778\u001b[0m       \u001b[35m0.7563\u001b[0m        0.4903  35.7642\n",
      "    189      \u001b[36m0.7647\u001b[0m        \u001b[32m0.4778\u001b[0m       0.7561        0.4903  36.3163\n",
      "    190      \u001b[36m0.7648\u001b[0m        \u001b[32m0.4778\u001b[0m       0.7562        0.4903  36.0699\n",
      "    191      0.7648        \u001b[32m0.4777\u001b[0m       0.7561        0.4903  36.0346\n",
      "    192      \u001b[36m0.7648\u001b[0m        \u001b[32m0.4777\u001b[0m       0.7562        0.4904  36.2238\n",
      "    193      \u001b[36m0.7648\u001b[0m        \u001b[32m0.4776\u001b[0m       0.7562        0.4904  35.9624\n",
      "    194      \u001b[36m0.7649\u001b[0m        \u001b[32m0.4776\u001b[0m       0.7561        0.4905  36.2237\n",
      "    195      0.7648        \u001b[32m0.4775\u001b[0m       0.7561        0.4905  36.0540\n",
      "    196      0.7648        \u001b[32m0.4775\u001b[0m       0.7560        0.4905  35.8101\n",
      "    197      0.7648        \u001b[32m0.4775\u001b[0m       0.7563        0.4904  36.1640\n",
      "    198      \u001b[36m0.7649\u001b[0m        \u001b[32m0.4774\u001b[0m       0.7562        0.4904  36.0417\n",
      "    199      \u001b[36m0.7649\u001b[0m        \u001b[32m0.4774\u001b[0m       0.7562        0.4905  36.1296\n",
      "    200      \u001b[36m0.7650\u001b[0m        \u001b[32m0.4773\u001b[0m       0.7562        0.4904  36.2157\n",
      "    201      0.7649        \u001b[32m0.4773\u001b[0m       0.7562        0.4903  35.7578\n",
      "    202      \u001b[36m0.7650\u001b[0m        \u001b[32m0.4773\u001b[0m       0.7563        0.4903  36.1990\n",
      "    203      \u001b[36m0.7651\u001b[0m        \u001b[32m0.4772\u001b[0m       0.7562        0.4903  36.2754\n",
      "    204      \u001b[36m0.7651\u001b[0m        \u001b[32m0.4772\u001b[0m       0.7562        0.4904  36.0889\n",
      "    205      0.7650        \u001b[32m0.4771\u001b[0m       0.7561        0.4904  36.1859\n",
      "    206      0.7650        \u001b[32m0.4771\u001b[0m       0.7562        0.4904  35.9681\n",
      "    207      \u001b[36m0.7651\u001b[0m        \u001b[32m0.4771\u001b[0m       0.7561        0.4904  36.1567\n",
      "    208      \u001b[36m0.7652\u001b[0m        \u001b[32m0.4770\u001b[0m       0.7561        0.4904  36.4758\n",
      "    209      \u001b[36m0.7652\u001b[0m        \u001b[32m0.4770\u001b[0m       0.7562        0.4903  35.7312\n",
      "    210      0.7652        \u001b[32m0.4770\u001b[0m       0.7562        0.4904  36.1003\n",
      "    211      \u001b[36m0.7652\u001b[0m        \u001b[32m0.4769\u001b[0m       0.7563        0.4903  35.9962\n",
      "    212      0.7651        \u001b[32m0.4769\u001b[0m       \u001b[35m0.7563\u001b[0m        \u001b[31m0.4902\u001b[0m  36.0532\n",
      "    213      \u001b[36m0.7652\u001b[0m        \u001b[32m0.4769\u001b[0m       0.7562        0.4903  36.3398\n",
      "    214      \u001b[36m0.7653\u001b[0m        \u001b[32m0.4768\u001b[0m       0.7562        0.4903  35.8434\n",
      "    215      \u001b[36m0.7654\u001b[0m        \u001b[32m0.4768\u001b[0m       0.7562        0.4903  36.0920\n",
      "    216      \u001b[36m0.7654\u001b[0m        \u001b[32m0.4768\u001b[0m       0.7563        0.4903  36.3955\n",
      "    217      \u001b[36m0.7654\u001b[0m        \u001b[32m0.4767\u001b[0m       0.7562        0.4903  34.9830\n",
      "    218      \u001b[36m0.7655\u001b[0m        \u001b[32m0.4767\u001b[0m       0.7562        0.4904  35.1405\n",
      "    219      0.7654        \u001b[32m0.4767\u001b[0m       0.7563        0.4903  34.1841\n",
      "    220      0.7654        \u001b[32m0.4766\u001b[0m       0.7562        0.4903  33.9491\n",
      "    221      \u001b[36m0.7655\u001b[0m        \u001b[32m0.4766\u001b[0m       0.7563        \u001b[31m0.4902\u001b[0m  34.6490\n",
      "    222      \u001b[36m0.7656\u001b[0m        \u001b[32m0.4765\u001b[0m       \u001b[35m0.7563\u001b[0m        0.4902  34.6645\n",
      "    223      0.7655        \u001b[32m0.4765\u001b[0m       0.7563        0.4902  34.7960\n",
      "    224      \u001b[36m0.7656\u001b[0m        \u001b[32m0.4765\u001b[0m       \u001b[35m0.7564\u001b[0m        \u001b[31m0.4902\u001b[0m  34.8672\n",
      "    225      0.7656        \u001b[32m0.4764\u001b[0m       0.7562        0.4902  34.8064\n",
      "    226      0.7655        \u001b[32m0.4764\u001b[0m       0.7562        0.4902  34.7409\n",
      "    227      \u001b[36m0.7656\u001b[0m        \u001b[32m0.4764\u001b[0m       0.7562        0.4903  34.7281\n",
      "    228      \u001b[36m0.7656\u001b[0m        \u001b[32m0.4764\u001b[0m       0.7562        0.4902  34.6703\n",
      "    229      \u001b[36m0.7657\u001b[0m        \u001b[32m0.4763\u001b[0m       0.7562        0.4902  34.6803\n",
      "    230      0.7656        \u001b[32m0.4763\u001b[0m       0.7563        0.4902  34.2622\n",
      "    231      0.7657        \u001b[32m0.4763\u001b[0m       0.7563        0.4902  34.9269\n",
      "    232      \u001b[36m0.7657\u001b[0m        \u001b[32m0.4762\u001b[0m       \u001b[35m0.7564\u001b[0m        0.4902  33.9108\n",
      "    233      0.7656        \u001b[32m0.4762\u001b[0m       \u001b[35m0.7564\u001b[0m        \u001b[31m0.4901\u001b[0m  34.8315\n",
      "    234      \u001b[36m0.7657\u001b[0m        \u001b[32m0.4762\u001b[0m       \u001b[35m0.7565\u001b[0m        0.4901  34.8357\n",
      "    235      \u001b[36m0.7657\u001b[0m        \u001b[32m0.4761\u001b[0m       0.7563        0.4902  34.4304\n",
      "    236      0.7657        \u001b[32m0.4761\u001b[0m       \u001b[35m0.7566\u001b[0m        \u001b[31m0.4900\u001b[0m  34.5308\n",
      "    237      \u001b[36m0.7657\u001b[0m        \u001b[32m0.4761\u001b[0m       \u001b[35m0.7566\u001b[0m        \u001b[31m0.4899\u001b[0m  34.7119\n",
      "    238      \u001b[36m0.7657\u001b[0m        \u001b[32m0.4760\u001b[0m       0.7566        \u001b[31m0.4899\u001b[0m  34.9732\n",
      "    239      0.7657        \u001b[32m0.4760\u001b[0m       0.7565        0.4900  34.6083\n",
      "    240      \u001b[36m0.7658\u001b[0m        \u001b[32m0.4760\u001b[0m       0.7564        0.4900  34.7902\n",
      "    241      0.7657        \u001b[32m0.4760\u001b[0m       \u001b[35m0.7567\u001b[0m        0.4900  34.6877\n",
      "    242      \u001b[36m0.7659\u001b[0m        \u001b[32m0.4759\u001b[0m       \u001b[35m0.7567\u001b[0m        \u001b[31m0.4899\u001b[0m  34.5644\n",
      "    243      \u001b[36m0.7659\u001b[0m        \u001b[32m0.4759\u001b[0m       \u001b[35m0.7567\u001b[0m        \u001b[31m0.4899\u001b[0m  34.0632\n",
      "    244      \u001b[36m0.7660\u001b[0m        \u001b[32m0.4759\u001b[0m       0.7566        0.4899  33.8716\n",
      "    245      0.7659        \u001b[32m0.4758\u001b[0m       \u001b[35m0.7568\u001b[0m        0.4899  34.5290\n",
      "    246      0.7659        \u001b[32m0.4758\u001b[0m       \u001b[35m0.7569\u001b[0m        \u001b[31m0.4898\u001b[0m  35.1448\n",
      "    247      0.7659        \u001b[32m0.4758\u001b[0m       0.7567        \u001b[31m0.4898\u001b[0m  34.6044\n",
      "    248      0.7659        \u001b[32m0.4758\u001b[0m       0.7567        \u001b[31m0.4897\u001b[0m  34.4777\n",
      "    249      \u001b[36m0.7660\u001b[0m        \u001b[32m0.4757\u001b[0m       0.7569        \u001b[31m0.4897\u001b[0m  34.8220\n",
      "    250      \u001b[36m0.7661\u001b[0m        \u001b[32m0.4757\u001b[0m       \u001b[35m0.7570\u001b[0m        \u001b[31m0.4897\u001b[0m  34.9404\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "save_params got an unexpected argument 'f', did you mean 'f_f'?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\matth\\Desktop\\COMP 4531\\final.ipynb Cell 12\u001b[0m line \u001b[0;36m<cell line: 23>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/matth/Desktop/COMP%204531/final.ipynb#X24sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m X_train, y_train \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack(X_train), torch\u001b[39m.\u001b[39mtensor(y_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/matth/Desktop/COMP%204531/final.ipynb#X24sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m best_model\u001b[39m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/matth/Desktop/COMP%204531/final.ipynb#X24sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m best_model\u001b[39m.\u001b[39;49msave_params(f\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mbest_model.pth\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\skorch\\net.py:2488\u001b[0m, in \u001b[0;36mNeuralNet.save_params\u001b[1;34m(self, f_params, f_optimizer, f_criterion, f_history, use_safetensors, **kwargs)\u001b[0m\n\u001b[0;32m   2485\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m_save_state_dict\u001b[39m(state_dict, f_name):\n\u001b[0;32m   2486\u001b[0m         torch\u001b[39m.\u001b[39msave(module\u001b[39m.\u001b[39mstate_dict(), f_name)\n\u001b[1;32m-> 2488\u001b[0m kwargs_module, kwargs_other \u001b[39m=\u001b[39m _check_f_arguments(\n\u001b[0;32m   2489\u001b[0m     \u001b[39m'\u001b[39m\u001b[39msave_params\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   2490\u001b[0m     f_params\u001b[39m=\u001b[39mf_params,\n\u001b[0;32m   2491\u001b[0m     f_optimizer\u001b[39m=\u001b[39mf_optimizer,\n\u001b[0;32m   2492\u001b[0m     f_criterion\u001b[39m=\u001b[39mf_criterion,\n\u001b[0;32m   2493\u001b[0m     f_history\u001b[39m=\u001b[39mf_history,\n\u001b[0;32m   2494\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   2496\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m kwargs_module \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kwargs_other:\n\u001b[0;32m   2497\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose:\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\skorch\\utils.py:755\u001b[0m, in \u001b[0;36m_check_f_arguments\u001b[1;34m(caller_name, **kwargs)\u001b[0m\n\u001b[0;32m    753\u001b[0m \u001b[39mfor\u001b[39;00m key, val \u001b[39min\u001b[39;00m kwargs\u001b[39m.\u001b[39mitems():\n\u001b[0;32m    754\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m key\u001b[39m.\u001b[39mstartswith(\u001b[39m'\u001b[39m\u001b[39mf_\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m--> 755\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m    756\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m{name}\u001b[39;00m\u001b[39m got an unexpected argument \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{key}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, did you mean \u001b[39m\u001b[39m'\u001b[39m\u001b[39mf_\u001b[39m\u001b[39m{key}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    757\u001b[0m             \u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mcaller_name, key\u001b[39m=\u001b[39mkey))\n\u001b[0;32m    759\u001b[0m     \u001b[39mif\u001b[39;00m val \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    760\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: save_params got an unexpected argument 'f', did you mean 'f_f'?"
     ]
    }
   ],
   "source": [
    "best_trial = study.best_trial\n",
    "best_params = best_trial.params\n",
    "\n",
    "X_train, y_train = zip(*[data for data in train_dataset])\n",
    "X_train, y_train = torch.stack(X_train), torch.tensor(y_train)\n",
    "best_trial.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Above error was fixed below, didn't want to re-run code above as it to 20 hours.\n",
    "import pickle\n",
    "\n",
    "with open(\"study.pkl\", \"wb\") as f:\n",
    "    pickle.dump(study, f)\n",
    "#best_model.save_params(f='best_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
